# 補足 統計と情報とAI

備考: この資料は下記を修正したものです。 

[link](https://github.com/ItaruKaneko/lecture_general/blob/main/2023hit-u/md/01_30_information.md)

# 情報という概念のはじまり

こんにち通信や記録に関する「内容」の量として「情報」という概念が認知されている。
では情報とは何だろうか?

## PCM 符号化の登場

1950年代にベルAT&Tが開発した。

PCM は音声をデジタル化して伝送する。

PCM のアナログ伝送とくらべたメリットは何か?

高音質、ではない。PCMの電話の音質はたいしてよくない。

## PCMは高性能か?

戦後通信需要が急拡大した。

1本の電話線でどれだけの通信を確保できるか。

まず、周波数分割でできるだけ多くの音声を伝送することが追及された。AMラジオと同じ原理。

それに対し PCM は時分割で回線を分離した。

Q 時分割と周波数分割のどちらが効率がよいか?
A それほど変わらない(はず)

例: 現在の携帯電話でも時分割, 周波数分割はともに用いられている。

## PCMの決定的なメリットは何か?

以下は想像。

PCMは原理的に混信がおきない。

これが最大のメリットだったのではないかと考えられる。

周波数分割方式では、情報は完全に分離できないので、他回線の会話がもれてしまう。PCMでは原理的に、他回線からの漏れが現実的には皆無とすることができる。情報理論の応用。

PCM の利点がチャンネル間の分離であるという証拠は見つからなかった。しかし初期の電話において、混信が重要な技術目標であったことはwikiの記述から推測される。

[Since the distances between wires were unequal, the inductances did not cancel out, leading to crosstalk, or signal leakage. The solution to this was twisted pairs, where the two wires for a circuit were transposed, or twisted, at regular intervals, balancing and thus canceling the inductance. AT&T first tested this successfully on its new New York to Philadelphia line in 1885, and began using it widely after 1891, when Carty worked out a basic theory of line transposition.](https://ethw.org/Telephone_Transmission)

またG.712 多重化伝送においても crosstalk が重要な設計目標だったと推測できる。
[ITU G.712 TRANSMISSION PERFORMANCE CHARACTERISTICS OF PULSE CODE MODULATION CHANNEL](https://www.itu.int/rec/dologin_pub.asp?lang=s&id=T-REC-G.712-199611-S!!PDF-E&type=items)

以上のようにPCMは混信を防ぐために誕生した。
その意味では PCM は ELSI と縁があったといえる。

## bit の誕生

当時メモリサイズなどは、「格納できる文字数」などで表現されていた。
シャノンは理論の数学的基礎として「情報量」を定義した。その際すでにチューキーが使っていた「bit」という呼称を採用した。

シャノンが定義した情報量は下記の通り。


$${\displaystyle H(P) = \sum_{x_{i}\in \Omega }P(x_{i}) \cdot I(x_{i}) }$$


$${I(x_{i})=P(x_{i})\log P(x_{i})}$$


## 情報と確率と統計の関係

シャノンは通信の数学的理論で、情報の数学的な定義を通信に応用したが、歴史を振り返るとこの情報量という測度はそれまでにエントロピーとしてとらえられていた測度と同じであり、また現代の科学理論では単に人間が通信で送るメッセージの量という以上に物理的な意味を持っている。

たとえば、ブラックホールの情報量は表面積に比例するとされている。
また、量子コンピュータでは情報は量子情報としてふるまう。


統計力学ではエントロピーが用いられるがこれは、統計力学が扱う粒子の情報量と等価だ。

$$ { S=k \log W} $$

そして現在のデータサイエンスは統計学に基づいた議論を行うがこの確率と統計が情報と直接な関係がある。

ベイズ推定はまさに観測された事象を情報源としてあらたな確率分布を推定する。

$$
P(H|D) = \frac{P(H) P(D|H)}{P(D)}
$$

事前分布に対し、観測された事象を知ることで、事後の分布を求めている。

これを情報理論的にとらえると、事前分布の情報に対し、観測された事象を情報が加わることで、事後の情報が得られている。

それぞれの理論が成立した年代をならべると以下のようになる。

1. ワットの新型蒸気機関, 1769
1. ベイズの定理, 1763
1. ボルツマン,熱平衡法則に関する力学的熱理論の第2主法則と確率計算の関係について, 1877
1. フーリエ, 熱の解析的理論, 1878
1. シャノン, 通信の数学的理論, 1948
1. データサイエンス、人工知能の勃興, 2000

このように理論としては情報および統計は長い時間をかけて理解され、理論が発展してきた.

1950からのコンピュータ技術の実用化によって, 機械による計算が可能になりその応用が加速した。
2000からはコンピュータとネットワーク、実世界の様々なデータがコンピュータにとりこまれることで、さらにそのメリットが拡大し、利用が加速した。

これまでにおこってきた「革新」が、それがもたらす利益がさらに革新を加速するというフィードバックがつよまることにより、近年ますます加速を始めた、ととらえることも可能だろう。









## 



